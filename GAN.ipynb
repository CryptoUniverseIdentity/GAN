{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "# !ls /home/aistudio/data\n",
    "\n",
    "# ! mkdir -p /home/aistudio/work/save_model/\n",
    "# ! mkdir -p /home/aistudio/work/data/\n",
    "# ! mkdir -p /home/aistudio/work/log_file/\n",
    "! mkdir -p /home/aistudio/work/train_image/\n",
    "! mkdir -p /home/aistudio/work/pic/\n",
    "# !unzip -o -d /home/aistudio/work/data/ /home/aistudio/data/data17962.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import paddle\n",
    "from paddle.io import Dataset\n",
    "import six\n",
    "from PIL import Image as PilImage\n",
    "import paddle.fluid as fluid\n",
    "from paddle.static import InputSpec\n",
    "paddle.enable_static()\n",
    "img_dim = 64\n",
    "\n",
    "'''准备数据，定义Reader()'''\n",
    "PATH = 'work/data/faces/'\n",
    "\n",
    "class DataGenerater(Dataset):\n",
    "    \"\"\"\n",
    "    数据集定义\n",
    "    \"\"\"\n",
    "    def __init__(self,path=PATH):\n",
    "        \"\"\"\n",
    "        构造函数\n",
    "        \"\"\"\n",
    "        super(DataGenerater, self).__init__()\n",
    "        self.dir = path\n",
    "        self.datalist = os.listdir(PATH)\n",
    "        self.image_size = (img_dim,img_dim)\n",
    "    \n",
    "    # 每次迭代时返回数据和对应的标签\n",
    "    def __getitem__(self, idx):\n",
    "        return self._load_img(self.dir + self.datalist[idx])\n",
    "\n",
    "    # 返回整个数据集的总数\n",
    "    def __len__(self):\n",
    "        return len(self.datalist)\n",
    "    \n",
    "    def _load_img(self, path):\n",
    "        \"\"\"\n",
    "        统一的图像处理接口封装，用于规整图像大小和通道\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img=path\n",
    "            crop_size = img_dim\n",
    "            img = Image.open(img)\n",
    "            # 等比例缩放和中心裁剪\n",
    "            width = img.size[0]\n",
    "            height = img.size[1]\n",
    "            if width < height:\n",
    "                ratio = width / crop_size\n",
    "                width = width / ratio\n",
    "                height = height / ratio\n",
    "                img = img.resize((int(width), int(height)), Image.ANTIALIAS)\n",
    "                height = height / 2\n",
    "                crop_size2 = crop_size / 2\n",
    "                box = (0, int(height - crop_size2), int(width), int(height + crop_size2))\n",
    "            else:\n",
    "                ratio = height / crop_size\n",
    "                height = height / ratio\n",
    "                width = width / ratio\n",
    "                img = img.resize((int(width), int(height)), Image.ANTIALIAS)\n",
    "                width = width / 2\n",
    "                crop_size2 = crop_size / 2\n",
    "                box = (int(width - crop_size2), 0, int(width + crop_size2), int(height))\n",
    "            img = img.crop(box)\n",
    "            img = img.resize((crop_size, crop_size), Image.ANTIALIAS)\n",
    "\n",
    "            # 把单通道图变成3通道\n",
    "            if len(img.getbands()) == 1:\n",
    "                img1 = img2 = img3 = img\n",
    "                img = Image.merge('RGB', (img1, img2, img3))\n",
    "\n",
    "            # 转换成numpy值\n",
    "            img = np.array(img).astype(np.float32)\n",
    "            # 转换成CHW\n",
    "            img = img.transpose((2, 0, 1))\n",
    "            # 转换成BGR\n",
    "            img = img[(2, 1, 0), :, :] / 255.0\n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataGenerater()\n",
    "imgs = paddle.static.data(name='img', shape=[None,3,img_dim,img_dim], dtype='float32')\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    train_dataset, \n",
    "    places=paddle.CPUPlace(), \n",
    "    feed_list = [imgs],\n",
    "    batch_size=128, \n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    use_buffer_reader=True,\n",
    "    use_shared_memory=False,\n",
    "    drop_last=True,\n",
    "    )\n",
    "train_loader()\n",
    "\n",
    "for batch_id, data in enumerate(train_loader()):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    try:\n",
    "        for i in range(64):\n",
    "            image = np.array(data[0][i])[0].transpose((2,1,0))\n",
    "            plt.subplot(8, 8, i + 1)\n",
    "            plt.imshow(image, vmin=-1, vmax=1)\n",
    "            plt.axis('off')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "        plt.suptitle('\\n Training Images',fontsize=30)\n",
    "        plt.show()\n",
    "        break\n",
    "    except IOError:\n",
    "        print(IOError)\n",
    "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2349: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
    "  if isinstance(obj, collections.Iterator):\n",
    "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2366: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
    "  return list(data) if isinstance(data, collections.MappingView) else data\n",
    "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
    "  'a.item() instead', DeprecationWarning, stacklevel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_initializer=paddle.nn.initializer.Normal(mean=0.0, std=0.02)\n",
    "bn_initializer=paddle.nn.initializer.Normal(mean=1.0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class Discriminator(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_1 = nn.Conv2D(\n",
    "            3,64,4,2,1,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"d_conv_weight_1_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.conv_2 = nn.Conv2D(\n",
    "            64,128,4,2,1,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"d_conv_weight_2_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.bn_2 = nn.BatchNorm2D(\n",
    "            128,\n",
    "            weight_attr=paddle.ParamAttr(name=\"d_2_bn_weight_\",initializer=bn_initializer),momentum=0.8\n",
    "            )\n",
    "        self.conv_3 = nn.Conv2D(\n",
    "            128,256,4,2,1,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"d_conv_weight_3_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.bn_3 = nn.BatchNorm2D(\n",
    "            256,\n",
    "            weight_attr=paddle.ParamAttr(name=\"d_3_bn_weight_\",initializer=bn_initializer),momentum=0.8\n",
    "            )\n",
    "        self.conv_4 = nn.Conv2D(\n",
    "            256,512,4,2,1,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"d_conv_weight_4_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.bn_4 = nn.BatchNorm2D(\n",
    "            512,\n",
    "            weight_attr=paddle.ParamAttr(name=\"d_4_bn_weight_\",initializer=bn_initializer),momentum=0.8\n",
    "            )\n",
    "        self.conv_5 = nn.Conv2D(\n",
    "            512,1,4,1,0,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"d_conv_weight_5_\",initializer=conv_initializer)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x,negative_slope=0.2)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = F.leaky_relu(x,negative_slope=0.2)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = F.leaky_relu(x,negative_slope=0.2)\n",
    "        x = self.conv_4(x)\n",
    "        x = self.bn_4(x)\n",
    "        x = F.leaky_relu(x,negative_slope=0.2)\n",
    "        x = self.conv_5(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv_1 = nn.Conv2DTranspose(\n",
    "            100,512,4,1,0,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"g_dconv_weight_1_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.bn_1 = nn.BatchNorm2D(\n",
    "            512,\n",
    "            weight_attr=paddle.ParamAttr(name=\"g_1_bn_weight_\",initializer=bn_initializer),momentum=0.8\n",
    "            )\n",
    "        self.conv_2 = nn.Conv2DTranspose(\n",
    "            512,256,4,2,1,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"g_dconv_weight_2_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.bn_2 = nn.BatchNorm2D(\n",
    "            256,\n",
    "            weight_attr=paddle.ParamAttr(name=\"g_2_bn_weight_\",initializer=bn_initializer),momentum=0.8\n",
    "            )\n",
    "        self.conv_3 = nn.Conv2DTranspose(\n",
    "            256,128,4,2,1,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"g_dconv_weight_3_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.bn_3 = nn.BatchNorm2D(\n",
    "            128,\n",
    "            weight_attr=paddle.ParamAttr(name=\"g_3_bn_weight_\",initializer=bn_initializer),momentum=0.8\n",
    "            )\n",
    "        self.conv_4 = nn.Conv2DTranspose(\n",
    "            128,64,4,2,1,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"g_dconv_weight_4_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.bn_4 = nn.BatchNorm2D(\n",
    "            64,\n",
    "            weight_attr=paddle.ParamAttr(name=\"g_4_bn_weight_\",initializer=bn_initializer),momentum=0.8\n",
    "            )\n",
    "        self.conv_5 = nn.Conv2DTranspose(\n",
    "            64,3,4,2,1,\n",
    "            bias_attr=False,weight_attr=paddle.ParamAttr(name=\"g_dconv_weight_5_\",initializer=conv_initializer)\n",
    "            )\n",
    "        self.tanh = paddle.nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_4(x)\n",
    "        x = self.bn_4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_5(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###损失函数\n",
    "loss = paddle.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "import warnings\n",
    "import paddle.optimizer as optim\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "img_dim = 64\n",
    "lr = 0.0002\n",
    "epoch = 130\n",
    "output = \"work/train_image/\"\n",
    "batch_size = 128\n",
    "G_DIMENSION = 100\n",
    "beta1=0.5\n",
    "beta2=0.999\n",
    "output_path = 'work/train_image/'\n",
    "device = paddle.set_device('gpu')\n",
    "paddle.disable_static(device)\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "netD = Discriminator()\n",
    "netG = Generator()\n",
    "optimizerD = optim.Adam(parameters=netD.parameters(), learning_rate=lr, beta1=beta1, beta2=beta2)\n",
    "optimizerG = optim.Adam(parameters=netG.parameters(), learning_rate=lr, beta1=beta1, beta2=beta2)\n",
    "\n",
    "###训练过程\n",
    "losses = [[], []]\n",
    "#plt.ion()\n",
    "now = 0\n",
    "for pass_id in range(epoch):\n",
    "    # enumerate()函数将一个可遍历的数据对象组合成一个序列列表\n",
    "    for batch_id, data in enumerate(train_loader()):\n",
    "        #训练判别器 \n",
    "        optimizerD.clear_grad()\n",
    "        real_cpu = data[0]\n",
    "        label = paddle.full((batch_size,1,1,1),real_label,dtype='float32')\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = loss(output,label)\n",
    "        errD_real.backward()\n",
    "        optimizerD.step()\n",
    "        optimizerD.clear_grad()\n",
    "\n",
    "        noise = paddle.randn([batch_size,G_DIMENSION,1,1],'float32')\n",
    "        fake = netG(noise)\n",
    "        label = paddle.full((batch_size,1,1,1),fake_label,dtype='float32')\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = loss(output,label)\n",
    "        errD_fake.backward()\n",
    "        optimizerD.step()\n",
    "        optimizerD.clear_grad()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        losses[0].append(errD.numpy()[0])\n",
    "        ###训练生成器\n",
    "        optimizerG.clear_grad()\n",
    "        noise = paddle.randn([batch_size,G_DIMENSION,1,1],'float32')\n",
    "        fake = netG(noise)\n",
    "        label = paddle.full((batch_size,1,1,1),real_label,dtype=np.float32,)\n",
    "        output = netD(fake)\n",
    "        errG = loss(output,label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        optimizerG.clear_grad()\n",
    "        \n",
    "        losses[1].append(errG.numpy()[0])\n",
    "        if batch_id % 100 == 0:\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path)\n",
    "            # 每轮的生成结果\n",
    "            generated_image = netG(noise).numpy()\n",
    "            imgs = []\n",
    "            plt.figure(figsize=(15,15))\n",
    "            try:\n",
    "                for i in range(64):\n",
    "                    image = generated_image[i].transpose()\n",
    "                    image = np.where(image > 0, image, 0)\n",
    "                    plt.subplot(8, 8, i + 1)\n",
    "                    plt.imshow(image, vmin=-1, vmax=1)\n",
    "                    plt.axis('off')\n",
    "                    plt.xticks([])\n",
    "                    plt.yticks([])\n",
    "                    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "                msg = 'Epoch ID={0} Batch ID={1} \\n\\n D-Loss={2} G-Loss={3}'.format(pass_id, batch_id, errD.numpy()[0], errG.numpy()[0])\n",
    "                plt.suptitle(msg,fontsize=20)\n",
    "                plt.draw()\n",
    "                plt.savefig('{}/{:04d}_{:04d}.png'.format(output_path, pass_id, batch_id),bbox_inches='tight')\n",
    "                plt.pause(0.01)\n",
    "                display.clear_output(wait=True)\n",
    "            except IOError:\n",
    "                print(IOError)\n",
    "    paddle.save(netG.state_dict(), \"work/generator.params\")\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "x = np.arange(len(losses[0]))\n",
    "plt.title('Generator and Discriminator Loss During Training')\n",
    "plt.xlabel('Number of Batch')\n",
    "plt.plot(x,np.array(losses[0]),label='D Loss')\n",
    "plt.plot(x,np.array(losses[1]),label='G Loss')\n",
    "plt.legend()\n",
    "plt.savefig('work/Generator and Discriminator Loss During Training.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = paddle.set_device('gpu')\n",
    "paddle.disable_static(device)\n",
    "try:\n",
    "    generate = Generator()\n",
    "    state_dict = paddle.load(\"work/generator.params\")\n",
    "    generate.set_state_dict(state_dict)\n",
    "    noise = paddle.randn([100,100,1,1],'float32')\n",
    "    generated_image = generate(noise).numpy()\n",
    "    for j in range(100):\n",
    "        image = generated_image[j].transpose()\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "        plt.savefig('work/pic/' + str(j + 1), bbox_inches='tight')\n",
    "        plt.close()\n",
    "except IOError:\n",
    "    print(IOError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in range(0,3):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img_root = 'work/train_image/'#这里写你的文件夹路径，比如：/home/youname/data/img/,注意最后一个文件夹要有斜杠\n",
    "fps = 1    #保存视频的FPS，可以适当调整\n",
    "size=(855,941)\n",
    "#可以用(*'DVIX')或(*'X264'),如果都不行先装ffmepg: sudo apt-get install ffmepg\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "videoWriter = cv2.VideoWriter('work/test.avi',fourcc,fps,size)#最后一个是保存图片的尺寸\n",
    "\n",
    "#for(i=1;i<471;++i)\n",
    "for i in range(0,128):\n",
    "    for j in range(0,4):\n",
    "        # print('{}{:04d}_{:04d}.png'.format(img_root, i, j))\n",
    "        frame = cv2.imread('{}{:04d}_{:04d}.png'.format(img_root, i, j*100))# (img_root+'0000_0'+j+'00.png')# '{}/{:04d}_{:04d}.png'.format(output_path, pass_id, batch_id)\n",
    "        videoWriter.write(frame)\n",
    "videoWriter.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
